# app/services/rag_service.py
import os
import logging
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.schema import Document

logger = logging.getLogger("soulstay.langchain_rag")


class LangChainRAGService:
    def __init__(self, collection_name="soulstay_feedback", use_gpt: bool = False):
        """
        LangChain ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
        - use_gpt=False : ìì²´ ëª¨ë¸ë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’)
        - use_gpt=True : OpenAIë¥¼ ë¬¸ì¥ ìì—°í™”ì— ë³´ì¡°ì ìœ¼ë¡œ ì‚¬ìš©
        """
        try:
            self.embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
            self.vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory="./chroma_langchain"
            )

            self.llm = None
            if use_gpt:
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.3,
                    openai_api_key=os.getenv("OPENAI_API_KEY")
                )

            logger.info(f"âœ… LangChain RAG ì´ˆê¸°í™” ì™„ë£Œ (GPT ì‚¬ìš©: {use_gpt})")

        except Exception as e:
            logger.exception(f"âŒ LangChain RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def add_document(self, text: str, metadata: dict = None):
        """ë¬¸ì„œë¥¼ LangChain VectorStoreì— ì¶”ê°€"""
        try:
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            chunks = splitter.split_text(text)
            docs = [Document(page_content=c, metadata=metadata or {}) for c in chunks]
            self.vectorstore.add_documents(docs)
            logger.info(f"ğŸ“š ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")
            return True
        except Exception as e:
            logger.exception(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def search(self, query: str, top_k: int = 3):
        """RAG ë²¡í„° ê²€ìƒ‰"""
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": top_k})
        return retriever.get_relevant_documents(query)

    def ask_with_context(self, query: str, top_k: int = 3):
        """ê²€ìƒ‰ + GPTë¡œ ë¬¸ë§¥ ì •ë¦¬ (ë³´ì¡°ìš©)"""
        if not self.llm:
            logger.warning("âš ï¸ GPT ë¹„í™œì„±í™” ìƒíƒœ, ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜")
            return self.search(query, top_k=top_k)

        retriever = self.vectorstore.as_retriever(search_kwargs={"k": top_k})
        qa_chain = RetrievalQA.from_chain_type(llm=self.llm, retriever=retriever, chain_type="stuff")
        answer = qa_chain.invoke({"query": query})
        return answer["result"]

RAGService = LangChainRAGService
