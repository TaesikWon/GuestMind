# app/services/response_service_kobart.py
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

class KoBARTResponseGenerator:
    """í•œêµ­ì–´ ë¬¸ë§¥í˜• ë‹µë³€ ìƒì„±ê¸° (KoBART ê¸°ë°˜)"""

    def __init__(self, model_name="gogamza/kobart-base-v2"):
        print("ğŸ”„ KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)
        print(f"âœ… KoBART ë¡œë“œ ì™„ë£Œ ({self.device})")

    def compose(self, text, emotion, cases=None):
        """ê°ì • + ìœ ì‚¬ì‚¬ë¡€ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        context = ""
        if cases:
            context = " ".join([c["text"] for c in cases[:3]])
        prompt = self._build_prompt(text, emotion, context)
        return self._generate_response(prompt)

    def _build_prompt(self, text, emotion, context):
        tone = {
            "positive": "ê°ì‚¬í•œ ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
            "negative": "ì‚¬ê³¼ì™€ ê³µê°ì´ ë‹´ê¸´ ë‹µë³€ì„ í•˜ì„¸ìš”.",
            "neutral": "ê³µì†í•˜ê³  ê°ê´€ì ì¸ ì–´ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        }.get(emotion, "ê³µì†í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.")

        prompt = (
            f"ê³ ê° í”¼ë“œë°±: {text}\n"
            f"ìœ ì‚¬ í”¼ë“œë°± ì°¸ê³ : {context}\n"
            f"ë‹µë³€ ì§€ì¹¨: {tone}\n"
            f"AI ì‘ë‹µ:"
        )
        return prompt

    def _generate_response(self, prompt):
        """KoBARTë¥¼ ì´ìš©í•´ ë‹µë³€ ë¬¸ì¥ ìƒì„±"""
        inputs = self.tokenizer([prompt], return_tensors="pt", truncation=True).to(self.device)
        output_ids = self.model.generate(
            **inputs,
            max_length=150,
            num_beams=4,
            repetition_penalty=2.0,
            no_repeat_ngram_size=3,
            early_stopping=True,
        )
        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return response.strip()
